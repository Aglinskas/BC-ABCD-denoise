{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ab9dff-d773-49cb-912a-6ae6504e3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  8 03:58:42 PM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d54a12-4725-4338-b580-2301a89dcd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/sccn/andromeda1/aglinska/BC-ABCD-denoise/Code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14796ecd-3ebc-4123-a466-40c785910f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ants\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43086ba9-8407-4dfd-94e3-05bc81f9a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(fn):\n",
    "    if os.path.exists(fn):\n",
    "        with open(fn, 'rb') as file:\n",
    "            loaded_dict = pickle.load(file)\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b24abf15-f7ee-4d26-bd6d-4c90291cf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_columns(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Computes the Pearson correlation between corresponding columns of two matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    arr1 (np.ndarray): First matrix of shape (370, 1000)\n",
    "    arr2 (np.ndarray): Second matrix of shape (370, 1000)\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: 1D array of correlations for each column (size 1000)\n",
    "    \"\"\"\n",
    "    # Ensure input arrays are numpy arrays\n",
    "    arr1 = np.asarray(arr1)\n",
    "    arr2 = np.asarray(arr2)\n",
    "    \n",
    "    # Subtract the mean of each column (normalize)\n",
    "    arr1_centered = arr1 - np.mean(arr1, axis=0)\n",
    "    arr2_centered = arr2 - np.mean(arr2, axis=0)\n",
    "    \n",
    "    # Compute the numerator (covariance)\n",
    "    numerator = np.sum(arr1_centered * arr2_centered, axis=0)\n",
    "    \n",
    "    # Compute the denominator (product of standard deviations)\n",
    "    denominator = np.sqrt(np.sum(arr1_centered**2, axis=0) * np.sum(arr2_centered**2, axis=0))\n",
    "    \n",
    "    # Compute the Pearson correlation for each column\n",
    "    correlation = numerator / denominator\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a001fdc6-6e47-418d-b7d8-a58cf34f0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regs(events_fn):\n",
    "    from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "\n",
    "    events = pd.read_csv(events_fn,delimiter='\\t')\n",
    "\n",
    "    t_r = 2.0 \n",
    "    n_scans = epi.shape[-1]\n",
    "    frame_times = (np.arange(n_scans) * t_r)\n",
    "\n",
    "    X1 = make_first_level_design_matrix(frame_times,events,drift_model=\"polynomial\",drift_order=3,hrf_model=\"SPM\") #\n",
    "\n",
    "    #face_reg = X1['face'].values\n",
    "    #place_reg = X1[['body', 'house', 'object', 'scene', 'scramble']].values.sum(axis=1)\n",
    "\n",
    "    face_reg = X1[['face','body']].values.sum(axis=1)\n",
    "    place_reg = X1[['house','scene']].values.sum(axis=1)\n",
    "    \n",
    "    return face_reg,place_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5fc3e72d-50ad-46bc-aa55-3e6abecfd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_corr2reg(epi,roi,reg,ofn):\n",
    "    epi_flat = epi.numpy().reshape(-1,n_scans)\n",
    "    gm_idx = roi.flatten()==1\n",
    "    std1_idx = epi_flat.std(axis=1)>1e-3\n",
    "    use_idx=gm_idx*std1_idx\n",
    "    voxel_array = epi_flat[use_idx,:]\n",
    "    voxel_array = (voxel_array-voxel_array.mean(axis=1)[:,np.newaxis]) / voxel_array.std(axis=1)[:,np.newaxis]\n",
    "    reg_arr = np.repeat(reg[:,np.newaxis], voxel_array.shape[0], axis=1)\n",
    "    corr_vals = correlate_columns(voxel_array.transpose(),reg_arr)\n",
    "    corr_arr = np.zeros(epi_flat.shape[0])\n",
    "    corr_arr[use_idx]=corr_vals\n",
    "    corr_nii = roi.new_image_like(corr_arr.reshape(roi.shape))\n",
    "    #ofn=os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'corr2face_S{s}_R{r}.nii.gz')\n",
    "    corr_nii.to_filename(ofn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "99c40165-e7b3-45bd-9f7d-2cf0318d9d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indir = '../Data/StudyForrest/fmriprep/'\n",
    "subs = [s for s in os.listdir(indir) if all((s.startswith('sub'),not s.endswith('.html')))]\n",
    "n = len(subs)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a5461b-36c5-489f-9058-83afedd007b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_fn_temp = '../Data/StudyForrest/fmriprep/{sub}/ses-localizer/func/{sub}_ses-localizer_task-objectcategories_run-{r}_bold_space-MNI152NLin2009cAsym_preproc.nii.gz'\n",
    "compcor_fn_temp = '../Data/StudyForrest/CompCor-maps-forrest/{sub}-COMPCOR-objectcategories_run-{r}.nii.gz'\n",
    "events_fn_temp = '../Data/StudyForrest/events/{sub}_ses-localizer_task-objectcategories_run-{r}_events.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c08a433c-870d-4c98-8057-2cd6040ddd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:30<00:00, 15.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for s in tqdm(range(14)):\n",
    "    for r in [1,2,3,4]:\n",
    "        sub = subs[s]\n",
    "\n",
    "        epi_fn = epi_fn_temp.format(sub=sub,r=r)\n",
    "        compcor_fn = compcor_fn_temp.format(sub=sub,r=r)\n",
    "        events_fn = events_fn_temp.format(sub=sub,r=r)\n",
    "        gm_fn = '../Data/StudyForrest/fmriprep/mask_roi.nii'\n",
    "\n",
    "        #print([os.path.exists(fn) for fn in [epi_fn,compcor_fn,events_fn]])\n",
    "        assert all([os.path.exists(fn) for fn in [epi_fn,compcor_fn,events_fn]]),'missing files'\n",
    "\n",
    "        epi = ants.image_read(epi_fn)\n",
    "        compcor = ants.image_read(compcor_fn)\n",
    "        roi = ants.image_read(gm_fn)\n",
    "\n",
    "        n_scans = epi.shape[-1]\n",
    "\n",
    "        face_reg,place_reg = get_regs(events_fn)\n",
    "        map_corr2reg(epi,roi,face_reg,os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2face_S{s}_R{r}.nii.gz'))\n",
    "        map_corr2reg(epi,roi,place_reg,os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2place_S{s}_R{r}.nii.gz'))\n",
    "        \n",
    "        map_corr2reg(compcor,roi,face_reg,os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2face_S{s}_R{r}.nii.gz'))\n",
    "        map_corr2reg(compcor,roi,place_reg,os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2place_S{s}_R{r}.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3b8bb361-d9d6-4ff8-ba2d-49877010cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 27.91it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 28.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for s in tqdm(range(14)):\n",
    "    image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2face_S{s}_R{r}.nii.gz')) for r in [1,2,3,4]]\n",
    "    image_avg = image_files[0].new_image_like(np.array([image_file.numpy() for image_file in image_files]).mean(axis=0))\n",
    "    image_avg.to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2face_S{s}_R_avg.nii.gz'))\n",
    "    \n",
    "for s in tqdm(range(14)):\n",
    "    image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2place_S{s}_R{r}.nii.gz')) for r in [1,2,3,4]]\n",
    "    image_avg = image_files[0].new_image_like(np.array([image_file.numpy() for image_file in image_files]).mean(axis=0))\n",
    "    image_avg.to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2place_S{s}_R_avg.nii.gz'))\n",
    "    \n",
    "image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2face_S{s}_R_avg.nii.gz')) for s in range(14)]\n",
    "image_avg =np.array([image_file.numpy() for image_file in image_files]).mean(axis=0)\n",
    "image_files[0].new_image_like(image_avg).to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/','preproc-corr2face_grand_average.nii.gz'))\n",
    "\n",
    "image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'preproc-corr2place_S{s}_R_avg.nii.gz')) for s in range(14)]\n",
    "image_avg =np.array([image_file.numpy() for image_file in image_files]).mean(axis=0)\n",
    "image_files[0].new_image_like(image_avg).to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/','preproc-corr2place_grand_average.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "65b3e5fb-1f57-4c6f-a8c1-271b11b9cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 28.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for s in tqdm(range(14)):\n",
    "    image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2face_S{s}_R{r}.nii.gz')) for r in [1,2,3,4]]\n",
    "    image_avg = image_files[0].new_image_like(np.array([image_file.numpy() for image_file in image_files]).mean(axis=0))\n",
    "    image_avg.to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2face_S{s}_R_avg.nii.gz'))\n",
    "    \n",
    "for s in tqdm(range(14)):\n",
    "    image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2place_S{s}_R{r}.nii.gz')) for r in [1,2,3,4]]\n",
    "    image_avg = image_files[0].new_image_like(np.array([image_file.numpy() for image_file in image_files]).mean(axis=0))\n",
    "    image_avg.to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2place_S{s}_R_avg.nii.gz'))\n",
    "    \n",
    "image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2face_S{s}_R_avg.nii.gz')) for s in range(14)]\n",
    "image_avg =np.array([image_file.numpy() for image_file in image_files]).mean(axis=0)\n",
    "image_files[0].new_image_like(image_avg).to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/','compcor-corr2face_grand_average.nii.gz'))\n",
    "\n",
    "image_files = [ants.image_read(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/',f'compcor-corr2place_S{s}_R_avg.nii.gz')) for s in range(14)]\n",
    "image_avg =np.array([image_file.numpy() for image_file in image_files]).mean(axis=0)\n",
    "image_files[0].new_image_like(image_avg).to_filename(os.path.join('../Data/StudyForrest/DeepCor-baselines-2/','compcor-corr2place_grand_average.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f613c833-6580-4bdc-995a-64efe256c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fb2a73bf-979f-4e10-a070-0a8cd7fc18b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b277c51-4711-48f9-9802-ad10f37d5381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "723c94c4-1bba-4ab2-805b-800b3094fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f347b743-55cb-4bd4-8e84-624eb441fc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c915cb3-6582-485e-8725-75d623192433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e01adb9-d23a-4dfd-a692-3b702b927b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17189c25-445f-4c91-b96a-7cbfdd849b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8956bd34-ed10-492a-b538-cbfcb0bfb349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc627cff-55ab-45fa-acb3-aae3e8ae21bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edec760-7818-4283-ade8-c4acea0f16c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f2daad-ccb9-4e1a-b058-d0cb581b5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Refactored DeepCorr CVAE Script for fMRI (Face vs Place task example)\n",
    "\n",
    "This script loads fMRI data and anatomical masks, preprocesses the data, trains a \n",
    "Conditional Variational Autoencoder (CVAE) to separate signal of interest (face-related ROI) \n",
    "from background noise signals, and outputs results including a diagnostic dashboard plot.\n",
    "\n",
    "User can customize file paths and key parameters in the section below.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# *** User-Defined Parameters ***\n",
    "# ------------------------------\n",
    "# Paths to input data (modify these as needed)\n",
    "epi_path = \"/path/to/subject_bold.nii.gz\"        # Path to 4D fMRI EPI data (NIfTI)\n",
    "anat_path = \"/path/to/subject_T1w.nii.gz\"        # Path to anatomical data (T1w image) or its segmentations\n",
    "roi_mask_path = \"/path/to/ffa_mask.nii.gz\"       # Path to ROI mask (e.g., FFA mask in MNI space)\n",
    "\n",
    "# Preprocessing parameter\n",
    "n_dummy = 8  # Number of initial dummy scans to discard (replace with mean)\n",
    "\n",
    "# CVAE hyperparameters\n",
    "cvae_params = {\n",
    "    \"latent_dim\": (16, 16),    # latent dimensions (z_dim, s_dim)\n",
    "    \"epochs\": 50,             # number of training epochs\n",
    "    \"batch_size\": 256,        # training batch size\n",
    "    \"learning_rate\": 1e-3,    # learning rate for optimizer\n",
    "    \"beta\": 0.001,            # beta weight for KLD loss term\n",
    "    # (Additional hyperparameters like gamma, delta, etc., can be added if needed)\n",
    "}\n",
    "\n",
    "# Output directory for results\n",
    "ofn_root = \"./deepcorr_output\"\n",
    "\n",
    "# ------------------------------\n",
    "# Helper classes and functions\n",
    "# ------------------------------\n",
    "def safe_mkdir(path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Autograd function that inverts the gradient (multiplies by -lambda) on the backward pass.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        # Forward pass outputs the input tensor as-is\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass: multiply incoming gradient by -lambda_\n",
    "        lambda_ = ctx.lambda_\n",
    "        grad_input = -ctx.lambda_ * grad_output\n",
    "        return grad_input, None  # No gradient for lambda\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer that reverses the gradient (with a scaling factor lambda) during backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for CVAE training, providing ROI (obs) and RONI (noise) samples.\n",
    "    Each sample consists of one ROI voxel timecourse and one randomly scaled noise voxel timecourse.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_data, noise_data):\n",
    "        \"\"\"\n",
    "        obs_data : np.ndarray of shape (n_roi_voxels, n_time) for ROI time series.\n",
    "        noise_data : np.ndarray of shape (n_noise_voxels, n_time) for RONI (noise) time series.\n",
    "        \"\"\"\n",
    "        self.obs = obs_data\n",
    "        self.noise = noise_data\n",
    "        # Ensure we have equal number of samples by limiting to the smaller set\n",
    "        self.n_samples = min(self.obs.shape[0], self.noise.shape[0])\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    def __getitem__(self, index):\n",
    "        # Get one ROI voxel timecourse and one noise voxel timecourse\n",
    "        obs_ts = self.obs[index]         # shape (n_time,)\n",
    "        noise_ts = self.noise[index]     # shape (n_time,)\n",
    "        # Apply random scaling to noise (data augmentation)\n",
    "        s = 2 * np.random.beta(4, 4)     # random scale factor from Beta(4,4) in [0,2]\n",
    "        noise_aug = s * noise_ts\n",
    "        # Stack time series with spatial coordinates channels will be handled after data loading (see train function)\n",
    "        # Here we just return the time series (as float32 for PyTorch)\n",
    "        return obs_ts.astype(np.float32), noise_aug.astype(np.float32)\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional Variational Autoencoder for disentangling target (signal-of-interest) and background noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, in_channels, in_dim, latent_dim, hidden_dims=None,\n",
    "                 beta=1.0, gamma=1.0, delta=1.0,\n",
    "                 scale_MSE_GM=1.0, scale_MSE_CF=1.0, scale_MSE_FG=1.0,\n",
    "                 do_disentangle=True):\n",
    "        \"\"\"\n",
    "        Initialize the CVAE model.\n",
    "        Parameters:\n",
    "        - conf (torch.Tensor): Tensor of shape (batch_size, n_confounds, n_time) containing confound signals (repeated for batch).\n",
    "        - in_channels (int): Number of input channels (1 for time series + 3 for spatial coordinates = 4).\n",
    "        - in_dim (int): Length of the time dimension (number of time points).\n",
    "        - latent_dim (tuple): (latent_dim_z, latent_dim_s) sizes for two latent subspaces.\n",
    "        - hidden_dims (list): Conv layer channel sizes for encoder (default [64,128,256,256]).\n",
    "        - beta, gamma, delta (float): weight factors for loss terms (beta for KLD, gamma/delta for optional losses).\n",
    "        - scale_MSE_GM, scale_MSE_CF, scale_MSE_FG (float): scaling factors for ROI, RONI, and FG reconstruction losses.\n",
    "        - do_disentangle (bool): whether to enforce disentanglement losses (confound correlation losses).\n",
    "        \"\"\"\n",
    "        super(cVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_dim_z = latent_dim[0]  # latent dimension for \"z\" (one factor, e.g. confound-related)\n",
    "        self.latent_dim_s = latent_dim[1]  # latent dimension for \"s\" (another factor, e.g. signal-of-interest)\n",
    "        self.in_channels = in_channels    # expected input channels (should be 4: value + x,y,z coords)\n",
    "        self.in_dim = in_dim              # number of time points\n",
    "        # Loss weights\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.scale_MSE_GM = scale_MSE_GM\n",
    "        self.scale_MSE_CF = scale_MSE_CF\n",
    "        self.scale_MSE_FG = scale_MSE_FG\n",
    "        self.do_disentangle = do_disentangle\n",
    "        # Confounds for conditioning (expand conf to shape [batch_size, conf_dim, time])\n",
    "        # We assume conf is already a torch tensor of shape (batch_size, n_confounds, in_dim)\n",
    "        self.confounds = conf.float()\n",
    "        # Gradient Reversal layer for adversarial loss on confounds\n",
    "        self.grl = GradientReversalLayer(lambda_=1.0)\n",
    "        # Decoder for confounds from z-latent\n",
    "        self.decoder_confounds_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=self.latent_dim_z, out_channels=128,\n",
    "                               kernel_size=self.in_dim, stride=1, bias=False),\n",
    "            nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=6, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Sigmoid()  # output 6 normalized confound signals\n",
    "        )\n",
    "        # Decoder for confounds from s-latent\n",
    "        self.decoder_confounds_s = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=self.latent_dim_s, out_channels=128,\n",
    "                               kernel_size=self.in_dim, stride=1, bias=False),\n",
    "            nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=6, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Build Encoder networks for z and s\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [64, 128, 256, 256]\n",
    "        # Compute required padding for full convolutional coverage of time dimension\n",
    "        pad, final_size, pad_out = compute_padding(self.in_dim)\n",
    "        self.pad = pad\n",
    "        self.final_size = final_size\n",
    "        self.pad_out = pad_out\n",
    "        # Encoder for z-latent\n",
    "        modules_z = []\n",
    "        in_ch = in_channels\n",
    "        for h_dim in hidden_dims:\n",
    "            modules_z.append(nn.Sequential(\n",
    "                nn.Conv1d(in_ch, out_channels=h_dim, kernel_size=3,\n",
    "                          stride=2, padding=int(self.pad[-(len(modules_z)+1)])),\n",
    "                nn.LeakyReLU()\n",
    "            ))\n",
    "            in_ch = h_dim\n",
    "        self.encoder_z = nn.Sequential(*modules_z)\n",
    "        # Linear layers for z latent parameters\n",
    "        self.fc_mu_z = nn.Linear(hidden_dims[-1] * int(self.final_size), self.latent_dim_z)\n",
    "        self.fc_var_z = nn.Linear(hidden_dims[-1] * int(self.final_size), self.latent_dim_z)\n",
    "        # Encoder for s-latent (similar structure)\n",
    "        modules_s = []\n",
    "        in_ch = in_channels\n",
    "        for h_dim in hidden_dims:\n",
    "            modules_s.append(nn.Sequential(\n",
    "                nn.Conv1d(in_ch, out_channels=h_dim, kernel_size=3,\n",
    "                          stride=2, padding=int(self.pad[-(len(modules_s)+1)])),\n",
    "                nn.LeakyReLU()\n",
    "            ))\n",
    "            in_ch = h_dim\n",
    "        self.encoder_s = nn.Sequential(*modules_s)\n",
    "        # Linear layers for s latent parameters\n",
    "        self.fc_mu_s = nn.Linear(hidden_dims[-1] * int(self.final_size), self.latent_dim_s)\n",
    "        self.fc_var_s = nn.Linear(hidden_dims[-1] * int(self.final_size), self.latent_dim_s)\n",
    "        # Build Decoder for reconstruction of input (ROI time series + coords)\n",
    "        hidden_dims.reverse()  # reverse for decoder\n",
    "        self.decoder_input = nn.Linear(self.latent_dim_z + self.latent_dim_s,\n",
    "                                       hidden_dims[0] * int(self.final_size))\n",
    "        modules_dec = []\n",
    "        # Build decoder layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules_dec.append(nn.Sequential(\n",
    "                nn.ConvTranspose1d(hidden_dims[i], hidden_dims[i+1],\n",
    "                                   kernel_size=3, stride=2,\n",
    "                                   padding=int(pad_out[-(len(modules_dec)+4)]),\n",
    "                                   output_padding=int(pad_out[-(len(modules_dec)+4)])),\n",
    "                nn.LeakyReLU()\n",
    "            ))\n",
    "        self.decoder = nn.Sequential(*modules_dec)\n",
    "        # Final decoder layer to reconstruct original channels (should output in_channels channels)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dims[-1], hidden_dims[-1],\n",
    "                               kernel_size=3, stride=2,\n",
    "                               padding=int(pad_out[-1]), output_padding=int(pad_out[-1])),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(hidden_dims[-1], out_channels=self.in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def encode_z(self, x: torch.Tensor):\n",
    "        \"\"\"Encode input into z-latent (confound-related) parameters.\"\"\"\n",
    "        result = self.encoder_z(x)            # [B, ..., *] convolution outputs\n",
    "        result = torch.flatten(result, start_dim=1)  # flatten to [B, latent_features]\n",
    "        mu = self.fc_mu_z(result)\n",
    "        log_var = self.fc_var_z(result)\n",
    "        return mu, log_var\n",
    "\n",
    "    def encode_s(self, x: torch.Tensor):\n",
    "        \"\"\"Encode input into s-latent (signal-related) parameters.\"\"\"\n",
    "        result = self.encoder_s(x)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        mu = self.fc_mu_s(result)\n",
    "        log_var = self.fc_var_s(result)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor):\n",
    "        \"\"\"Sample from Gaussian distribution via reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward_tg(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass treating input as target (ROI) data.\n",
    "        Returns [reconstruction, input, tg_mu_z, tg_log_var_z, tg_mu_s, tg_log_var_s, tg_z, tg_s]\n",
    "        \"\"\"\n",
    "        tg_mu_z, tg_log_var_z = self.encode_z(x)\n",
    "        tg_mu_s, tg_log_var_s = self.encode_s(x)\n",
    "        tg_z = self.reparameterize(tg_mu_z, tg_log_var_z)\n",
    "        tg_s = self.reparameterize(tg_mu_s, tg_log_var_s)\n",
    "        # Reconstruction: sum of foreground and background reconstructions\n",
    "        recon = self.forward_bg(x)[0] + self.forward_fg(x)[0]\n",
    "        return recon, x, tg_mu_z, tg_log_var_z, tg_mu_s, tg_log_var_s, tg_z, tg_s\n",
    "\n",
    "    def forward_fg(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass for foreground (signal-of-interest) only.\n",
    "        Returns [foreground_reconstruction, input, tg_mu_s, tg_log_var_s]\n",
    "        \"\"\"\n",
    "        tg_mu_s, tg_log_var_s = self.encode_s(x)\n",
    "        tg_s = self.reparameterize(tg_mu_s, tg_log_var_s)\n",
    "        # Zero-out z latent to reconstruct only signal-related components\n",
    "        zeros_z = torch.zeros(tg_s.size(0), self.latent_dim_z, device=x.device)\n",
    "        fg_out = self.decode(torch.cat((zeros_z, tg_s), dim=1))\n",
    "        return fg_out, x, tg_mu_s, tg_log_var_s\n",
    "\n",
    "    def forward_bg(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass for background (noise) only.\n",
    "        Returns [background_reconstruction, input, bg_mu_z, bg_log_var_z]\n",
    "        \"\"\"\n",
    "        bg_mu_z, bg_log_var_z = self.encode_z(x)\n",
    "        bg_z = self.reparameterize(bg_mu_z, bg_log_var_z)\n",
    "        # Zero-out s latent to reconstruct only confound/background components\n",
    "        zeros_s = torch.zeros(bg_z.size(0), self.latent_dim_s, device=x.device)\n",
    "        bg_out = self.decode(torch.cat((bg_z, zeros_s), dim=1))\n",
    "        return bg_out, x, bg_mu_z, bg_log_var_z\n",
    "\n",
    "    def ncc(self, x: torch.Tensor, y: torch.Tensor, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Compute (1 - normalized cross-correlation) between two inputs. Outputs a distance (lower is more correlated).\n",
    "        \"\"\"\n",
    "        # Flatten spatial (or voxel) dimensions to compare along time\n",
    "        x_flat = x.flatten(start_dim=1)\n",
    "        y_flat = y.flatten(start_dim=1)\n",
    "        x_mean = x_flat.mean(dim=1, keepdim=True)\n",
    "        y_mean = y_flat.mean(dim=1, keepdim=True)\n",
    "        x_std = x_flat.std(dim=1, keepdim=True) + eps\n",
    "        y_std = y_flat.std(dim=1, keepdim=True) + eps\n",
    "        ncc_val = ((x_flat - x_mean) * (y_flat - y_mean) / (x_std * y_std)).mean(dim=1)\n",
    "        return 1 - ncc_val  # return 1 - correlation (to be minimized, i.e., high correlation -> low loss)\n",
    "\n",
    "    def loss_function(self, *args):\n",
    "        \"\"\"\n",
    "        Compute CVAE loss components and return as a dictionary.\n",
    "        Expects args in order: \n",
    "         [recons_tg, input_tg, tg_mu_z, tg_log_var_z, tg_mu_s, tg_log_var_s, tg_z, tg_s,\n",
    "          recons_bg, input_bg, bg_mu_z, bg_log_var_z]\n",
    "        \"\"\"\n",
    "        # Unpack inputs\n",
    "        recons_tg, input_tg = args[0], args[1]\n",
    "        tg_mu_z, tg_log_var_z = args[2], args[3]\n",
    "        tg_mu_s, tg_log_var_s = args[4], args[5]\n",
    "        tg_z, tg_s = args[6], args[7]\n",
    "        recons_bg, input_bg = args[8], args[9]\n",
    "        bg_mu_z, bg_log_var_z = args[10], args[11]\n",
    "        # Reconstruction losses (MSE for ROI and RONI signals, first channel is the time series value)\n",
    "        recons_loss_roi = F.mse_loss(recons_tg[:, 0, :], input_tg[:, 0, :]) * self.scale_MSE_GM\n",
    "        recons_loss_roni = F.mse_loss(recons_bg[:, 0, :], input_bg[:, 0, :]) * self.scale_MSE_CF\n",
    "        recons_loss = recons_loss_roi + recons_loss_roni\n",
    "        # Additional reconstruction of foreground from ROI input (for FG vs BG consistency)\n",
    "        recons_fg = self.forward_fg(input_tg)[0]\n",
    "        # Adversarial confound reconstruction losses\n",
    "        conf_pred_z = self.decoder_confounds_z(tg_z.unsqueeze(2))  # shape [B,6,T]\n",
    "        conf_pred_s = self.decoder_confounds_s(tg_s.unsqueeze(2))\n",
    "        loss_recon_conf_s = self.grl(F.mse_loss(conf_pred_s, self.confounds)) * 1e2   # maximize MSE (via GRL) for conf from s-latent\n",
    "        loss_recon_conf_z = F.mse_loss(conf_pred_z, self.confounds) * 1e2             # minimize MSE for conf from z-latent\n",
    "        # Encourage reconstructed ROI and RONI to be correlated with original (minimize 1-NCC)\n",
    "        ncc_loss_tg = self.ncc(input_tg, recons_tg).mean() * 1.0\n",
    "        ncc_loss_bg = self.ncc(input_bg, recons_bg).mean() * 1.0\n",
    "        # Encourage confound outputs to (not) correlate with true confounds (non-correlations constraint, NCC)\n",
    "        ncc_loss_conf_s = 0.0\n",
    "        for i in range(self.confounds.shape[1]):\n",
    "            ncc_loss_conf_s += self.ncc(self.confounds[:, i, :], conf_pred_s[:, i, :]).mean() * 1e1\n",
    "        ncc_loss_conf_s = self.grl(ncc_loss_conf_s)  # apply GRL (maximize correlation, i.e., minimize 1-corr via neg. gradient)\n",
    "        ncc_loss_conf_z = 0.0\n",
    "        for i in range(self.confounds.shape[1]):\n",
    "            ncc_loss_conf_z += self.ncc(self.confounds[:, i, :], conf_pred_z[:, i, :]).mean() * 1e1\n",
    "        # Encourage foreground and background parts to complement each other (smoothness and orthogonality)\n",
    "        recond_bg = self.forward_bg(input_tg)[0]  # background reconstruction of ROI input\n",
    "        fg_bg_ncc = self.ncc(recond_bg[:, 0, :], recons_fg[:, 0, :]).mean()\n",
    "        recons_loss_fg = F.mse_loss(torch.zeros_like(fg_bg_ncc), 1 - fg_bg_ncc) * 1e4  # force FG and BG to be correlated (minimize 1-corr)\n",
    "        # Small additional penalties to encourage consistency\n",
    "        recons_loss += F.mse_loss(recons_fg[:, 0, :], input_tg[:, 0, :]) * 1e-4  # tiny weight on FG matching ROI\n",
    "        recons_loss += F.mse_loss(recons_bg[:, 0, :], input_bg[:, 0, :]) * 1e-5  # tiny weight on BG matching RONI\n",
    "        # Smoothness loss on temporal derivatives of FG and BG outputs (encourage smooth timecourses)\n",
    "        smoothness_loss = torch.mean((recons_fg[:, 0, :, 1:] - recons_fg[:, 0, :, :-1])**2) if recons_fg.dim() == 3 else \\\n",
    "                          torch.mean((recons_fg[:, 0, 1:] - recons_fg[:, 0, :-1])**2)\n",
    "        smoothness_loss += torch.mean((recond_bg[:, 0, 1:] - recond_bg[:, 0, :-1])**2)\n",
    "        smoothness_loss = smoothness_loss * 0.01\n",
    "        # (Optional additional losses can be added here if needed; currently not used)\n",
    "        # KLD loss (average over batch)\n",
    "        kld_loss_z = -0.5 * torch.mean(1 + tg_log_var_z - tg_mu_z**2 - torch.exp(tg_log_var_z))\n",
    "        kld_loss_s = -0.5 * torch.mean(1 + tg_log_var_s - tg_mu_s**2 - torch.exp(tg_log_var_s))\n",
    "        kld_loss_bg = -0.5 * torch.mean(1 + bg_log_var_z - bg_mu_z**2 - torch.exp(bg_log_var_z))\n",
    "        kld_loss = (kld_loss_z + kld_loss_s + kld_loss_bg) / 3.0\n",
    "        kld_loss = kld_loss * self.beta\n",
    "        # Sum all loss components for total loss\n",
    "        total_loss = recons_loss + kld_loss + recons_loss_fg + ncc_loss_tg + ncc_loss_bg + loss_recon_conf_s + loss_recon_conf_z + smoothness_loss + ncc_loss_conf_z + ncc_loss_conf_s\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'kld_loss': kld_loss,\n",
    "            'recons_loss_roi': recons_loss_roi,\n",
    "            'recons_loss_roni': recons_loss_roni,\n",
    "            'loss_recon_conf_s': loss_recon_conf_s,\n",
    "            'loss_recon_conf_z': loss_recon_conf_z,\n",
    "            'ncc_loss_tg': ncc_loss_tg,\n",
    "            'ncc_loss_bg': ncc_loss_bg,\n",
    "            'ncc_loss_conf_s': ncc_loss_conf_s * 0.0,  # effectively 0 (not used in final loss)\n",
    "            'ncc_loss_conf_z': ncc_loss_conf_z,\n",
    "            'smoothness_loss': smoothness_loss,\n",
    "            'recons_loss_fg': recons_loss_fg\n",
    "        }\n",
    "\n",
    "def compute_in(x):\n",
    "    \"\"\"Compute one iteration of \"in\" size for padding calculation.\"\"\"\n",
    "    return (x - 3) / 2 + 1\n",
    "\n",
    "def compute_in_size(x):\n",
    "    \"\"\"Compute size after 4 downsampling (conv) layers given initial length x.\"\"\"\n",
    "    for _ in range(4):\n",
    "        x = compute_in(x)\n",
    "    return x\n",
    "\n",
    "def compute_out_size(x):\n",
    "    \"\"\"Inverse of compute_in_size: compute output length after 4 upsampling layers given latent size x.\"\"\"\n",
    "    # The upsampling sequence (convtranspose) is roughly the inverse of conv\n",
    "    return ((((x * 2 + 1) * 2 + 1) * 2 + 1) * 2 + 1)\n",
    "\n",
    "def compute_padding(x):\n",
    "    \"\"\"\n",
    "    Compute padding and output padding required for 4 conv downsampling and 4 convtranspose upsampling layers.\n",
    "    Returns:\n",
    "      pad (str): binary string of padding bits for conv layers,\n",
    "      final_size (float): final latent feature size after conv layers,\n",
    "      pad_out (str): binary string of output padding bits for convtranspose layers.\n",
    "    \"\"\"\n",
    "    # Determine padding for conv layers\n",
    "    rounding = np.ceil(compute_in_size(x)) - compute_in_size(x)\n",
    "    y = ((((rounding * 2) * 2) * 2) * 2)\n",
    "    pad = bin(int(y)).replace('0b', '').zfill(4)  # binary representation of which convs need padding\n",
    "    final_size = compute_in_size(x + y)  # final latent feature length\n",
    "    # Determine output padding for convtranspose to reach original size\n",
    "    pad_out = bin(int(compute_out_size(final_size) - x)).replace('0b', '').zfill(4)\n",
    "    return pad, final_size, pad_out\n",
    "\n",
    "def correlate_columns(arr1: np.ndarray, arr2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute column-wise Pearson correlation between two arrays.\n",
    "    Both arrays should be shape (n_time, n_features). Returns 1D array of correlations for each feature column.\n",
    "    \"\"\"\n",
    "    arr1 = np.asarray(arr1)\n",
    "    arr2 = np.asarray(arr2)\n",
    "    # Subtract mean from each column\n",
    "    arr1_centered = arr1 - np.mean(arr1, axis=0)\n",
    "    arr2_centered = arr2 - np.mean(arr2, axis=0)\n",
    "    # Compute Pearson correlation for each column\n",
    "    numerator = np.sum(arr1_centered * arr2_centered, axis=0)\n",
    "    denominator = np.sqrt(np.sum(arr1_centered**2, axis=0) * np.sum(arr2_centered**2, axis=0))\n",
    "    correlation = numerator / (denominator + 1e-8)\n",
    "    return correlation\n",
    "\n",
    "def interpolate_outliers(data: np.ndarray, outlier_mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    Linearly interpolate over motion outlier frames in a (n_voxels x n_time) array.\n",
    "    outlier_mask is a boolean array of length n_time, True at indices to interpolate.\n",
    "    Returns a corrected array with outlier frames replaced by interpolation of nearest valid frames.\n",
    "    \"\"\"\n",
    "    t = np.arange(data.shape[1])\n",
    "    good = ~outlier_mask\n",
    "    bad = outlier_mask\n",
    "    corrected = data.copy()\n",
    "    if np.sum(bad) == 0:\n",
    "        return corrected  # no outliers to interpolate\n",
    "    t_good = t[good]\n",
    "    t_bad = t[bad]\n",
    "    # Interpolate for each voxel's time series\n",
    "    for v in range(data.shape[0]):\n",
    "        ts = data[v, :]\n",
    "        y_good = ts[good]\n",
    "        # Linear interpolation for bad frames\n",
    "        y_interp = np.interp(t_bad, t_good, y_good)\n",
    "        corrected[v, bad] = y_interp\n",
    "    return corrected\n",
    "\n",
    "def load_data_and_preprocess(epi_path: str, anat_path: str, n_dummy: int):\n",
    "    \"\"\"\n",
    "    Load fMRI data and anatomical masks, apply preprocessing, and return numpy arrays for model training.\n",
    "    Returns:\n",
    "      obs_list (np.ndarray): ROI time series data (voxels x time)\n",
    "      noi_list (np.ndarray): RONI (noise) time series data (voxels x time)\n",
    "      conf (np.ndarray): Confound regressor matrix (n_confounds x time)\n",
    "      ffa (np.ndarray): FFA ROI time series (voxels_in_FFA x time)\n",
    "      face_reg (np.ndarray): Task regressor time course for \"face\" condition (length = n_time)\n",
    "      place_reg (np.ndarray): Task regressor time course for \"place\" condition (length = n_time)\n",
    "      ffa_compcorr (np.ndarray): FFA time series after CompCor (baseline noise removal using PCA of RONI)\n",
    "    \"\"\"\n",
    "    # Import nibabel or ANTs for image loading\n",
    "    try:\n",
    "        import ants  # ANTsPy for NIfTI\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"ANTs library not found. Please install antspyx or use nibabel for image I/O.\")\n",
    "    # Read fMRI 4D image\n",
    "    epi_img = ants.image_read(epi_path)\n",
    "    epi_data = epi_img.numpy()  # numpy array shape (X, Y, Z, T)\n",
    "    # If anatomical segmentation files exist (GM, WM, CSF), read them; otherwise try to derive from anat_path\n",
    "    # Here we assume anat_path is the structural image; segmentation files with standard names are in same folder\n",
    "    anat_dir = os.path.dirname(anat_path)\n",
    "    base_name = os.path.basename(anat_path)\n",
    "    # Construct expected segmentation file names (for fMRIPrep outputs)\n",
    "    gm_prob_path = os.path.join(anat_dir, base_name.replace(\"T1w\", \"GM_probseg\"))\n",
    "    wm_prob_path = os.path.join(anat_dir, base_name.replace(\"T1w\", \"WM_probseg\"))\n",
    "    csf_prob_path = os.path.join(anat_dir, base_name.replace(\"T1w\", \"CSF_probseg\"))\n",
    "    # If those do not exist, raise error (for simplicity, require segmented probability maps)\n",
    "    if not (os.path.exists(gm_prob_path) and os.path.exists(wm_prob_path) and os.path.exists(csf_prob_path)):\n",
    "        raise FileNotFoundError(\"Grey matter/White matter/CSF probability maps not found. Please provide segmentation files.\")\n",
    "    gm_img = ants.image_read(gm_prob_path)\n",
    "    wm_img = ants.image_read(wm_prob_path)\n",
    "    csf_img = ants.image_read(csf_prob_path)\n",
    "    gm_data = gm_img.numpy()\n",
    "    wm_data = wm_img.numpy()\n",
    "    csf_data = csf_img.numpy()\n",
    "    # Threshold probability maps to create binary masks for ROI (GM) and RONI (WM+CSF)\n",
    "    gm_mask = gm_data > 0.5\n",
    "    cf_mask = (wm_data + csf_data) > 0.5  # \"cf\" mask is combined WM and CSF (potential confound regions)\n",
    "    # Ensure no overlap between gm_mask and cf_mask\n",
    "    overlap = gm_mask & cf_mask\n",
    "    if overlap.any():\n",
    "        gm_mask[overlap] = False\n",
    "        cf_mask[overlap] = False\n",
    "    # Remove voxels with near-zero variance from masks (these are typically outside brain or static areas)\n",
    "    # Compute temporal std dev for each voxel:\n",
    "    epi_time_std = epi_data.std(axis=-1)\n",
    "    gm_mask = gm_mask & (epi_time_std > 1e-3)\n",
    "    cf_mask = cf_mask & (epi_time_std > 1e-3)\n",
    "    # Flatten spatial data for time series extraction\n",
    "    nx, ny, nz, nt = epi_data.shape\n",
    "    epi_flat = epi_data.reshape(-1, nt).T  # shape (T, Nvoxels)\n",
    "    # Handle dummy scans: replace first n_dummy volumes with the mean of remaining volumes (to avoid abrupt transient)\n",
    "    if n_dummy > 0 and n_dummy < nt:\n",
    "        mean_vol = epi_flat[n_dummy:, :].mean(axis=0)\n",
    "        epi_flat[:n_dummy, :] = mean_vol\n",
    "    # Revert shape to (Nvoxels, T) for easier indexing by mask\n",
    "    epi_flat = epi_flat.T  # shape (Nvoxels, T)\n",
    "    # Flatten masks to match epi_flat indexing\n",
    "    gm_flat = gm_mask.flatten()\n",
    "    cf_flat = cf_mask.flatten()\n",
    "    # Extract ROI (GM) and RONI (WM+CSF) time series\n",
    "    func_gm = epi_flat[gm_flat, :]  # ROI signals (shape: n_gm_voxels x T)\n",
    "    func_cf = epi_flat[cf_flat, :]  # Noise signals (shape: n_cf_voxels x T)\n",
    "    # Load confound TSV file if exists (assuming fMRIPrep confounds file naming)\n",
    "    conf_fn = epi_path.replace(\"desc-preproc_bold.nii.gz\", \"desc-confounds_timeseries.tsv\")\n",
    "    conf = None\n",
    "    if os.path.exists(conf_fn):\n",
    "        df_conf = pd.read_csv(conf_fn, sep='\\t')\n",
    "        # Use 6 motion parameters as confounds (trans_x,y,z, rot_x,y,z) if available\n",
    "        motion_cols = [c for c in ['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z'] if c in df_conf.columns]\n",
    "        if motion_cols:\n",
    "            conf = df_conf.loc[:, motion_cols].values.T  # shape (6, T)\n",
    "        else:\n",
    "            # If not found, use all numeric confounds as a fallback (excluding non-numeric columns)\n",
    "            conf = df_conf.select_dtypes(include=[np.number]).values.T\n",
    "    else:\n",
    "        # If no confound file, create a dummy zero matrix for conf (to satisfy model input)\n",
    "        conf = np.zeros((1, func_gm.shape[1]))\n",
    "    # Identify motion outlier frames using framewise displacement if available\n",
    "    fd_mask = None\n",
    "    if 'framewise_displacement' in locals() or ('framewise_displacement' in df_conf.columns):\n",
    "        fd_vals = df_conf['framewise_displacement'].fillna(0.0).values if 'df_conf' in locals() else None\n",
    "        if fd_vals is not None:\n",
    "            fd_mask = fd_vals > 0.25  # True for frames with FD > 0.25 (could be considered outliers)\n",
    "    if fd_mask is None:\n",
    "        fd_mask = np.zeros(func_gm.shape[1], dtype=bool)\n",
    "    # Interpolate over outlier frames in ROI and RONI data\n",
    "    func_gm = interpolate_outliers(func_gm, fd_mask)\n",
    "    func_cf = interpolate_outliers(func_cf, fd_mask)\n",
    "    # Remove any voxels from ROI and RONI that became zero-variance after interpolation\n",
    "    gm_var_mask = func_gm.std(axis=1) > 1e-3\n",
    "    cf_var_mask = func_cf.std(axis=1) > 1e-3\n",
    "    func_gm = func_gm[gm_var_mask, :]\n",
    "    func_cf = func_cf[cf_var_mask, :]\n",
    "    # Also update coordinates lists accordingly (we will compute coordinates next for ones we kept)\n",
    "    # Compute coordinates of all voxels for reference\n",
    "    x_idx, y_idx, z_idx = np.meshgrid(np.arange(nx), np.arange(ny), np.arange(nz), indexing='ij')\n",
    "    coords_flat = np.stack([x_idx.flatten(), y_idx.flatten(), z_idx.flatten()], axis=1)\n",
    "    gm_coords = coords_flat[gm_flat]         # coordinates for original ROI voxels\n",
    "    cf_coords = coords_flat[cf_flat]         # coordinates for original RONI voxels\n",
    "    gm_coords = gm_coords[gm_var_mask]       # apply mask of removed low-variance ROI voxels\n",
    "    cf_coords = cf_coords[cf_var_mask]       # apply mask of removed low-variance RONI voxels\n",
    "    # Load events file if exists (to compute task regressors)\n",
    "    # Assuming BIDS format events file corresponding to epi_path\n",
    "    events_fn = epi_path.split('_desc-')[0] + '_events.tsv'\n",
    "    face_reg = None\n",
    "    place_reg = None\n",
    "    if os.path.exists(events_fn):\n",
    "        events_df = pd.read_csv(events_fn, sep='\\t')\n",
    "        # Create design matrix with SPM HRF for all condition columns\n",
    "        from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "        tr = getattr(epi_img, \"spacing\", [None,None,None, None])[3] or 0.8  # try to get TR, default 0.8s if not found\n",
    "        n_scans = func_gm.shape[1]\n",
    "        frame_times = np.arange(n_scans) * tr\n",
    "        design_matrix = make_first_level_design_matrix(frame_times, events_df,\n",
    "                                                       drift_model=\"polynomial\", drift_order=3, hrf_model=\"SPM\")\n",
    "        # For the N-back face vs place task, sum face conditions and place conditions\n",
    "        face_columns = [col for col in design_matrix.columns if 'face' in col.lower()]\n",
    "        place_columns = [col for col in design_matrix.columns if 'place' in col.lower()]\n",
    "        if face_columns:\n",
    "            face_reg = design_matrix[face_columns].values.sum(axis=1)\n",
    "        if place_columns:\n",
    "            place_reg = design_matrix[place_columns].values.sum(axis=1)\n",
    "    if face_reg is None:\n",
    "        # If events not found or no face/place columns, use zeros (or could raise warning)\n",
    "        face_reg = np.zeros(func_gm.shape[1])\n",
    "        place_reg = np.zeros(func_gm.shape[1])\n",
    "    # Identify FFA ROI voxels within the ROI mask\n",
    "    ffa_mask_img = ants.image_read(roi_mask_path)\n",
    "    ffa_mask_data = ffa_mask_img.numpy()\n",
    "    # Select voxels that are in both FFA mask and GM mask\n",
    "    ffa_mask_flat = ffa_mask_data.flatten()\n",
    "    ffa_idx_all = (ffa_mask_flat == 1) & gm_flat  # boolean mask for FFA voxels among all voxels\n",
    "    # Now map this to indices in our reduced ROI set (after dropping low-var voxels)\n",
    "    # We have gm_flat (before drop) and gm_var_mask (after drop). We need indices in func_gm (after drop).\n",
    "    gm_indices_all = np.where(gm_flat)[0]   # indices (in flattened image) of ROI voxels\n",
    "    ffa_indices_all = np.where(ffa_idx_all)[0]   # indices (in flattened image) of FFA voxels\n",
    "    # Determine which of those FFA indices survived low-variance filtering:\n",
    "    ffa_survived = np.isin(gm_indices_all[gm_var_mask], ffa_indices_all)\n",
    "    # Extract FFA time series from the filtered ROI data\n",
    "    ffa_data = func_gm[ffa_survived, :]  # shape (n_ffa_voxels, T)\n",
    "    # Compute baseline CompCor for FFA: use top 5 PCA components from noise (RONI) to regress out from FFA signals\n",
    "    n_components = min(5, func_cf.shape[0], func_cf.shape[1])\n",
    "    if n_components < 1:\n",
    "        # If noise region is empty or too small, skip compcorr\n",
    "        ffa_comp = ffa_data.copy()\n",
    "    else:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        conf_pcs = pca.fit_transform(func_cf.T)  # shape (T x n_components)\n",
    "        lin_reg = linear_model.LinearRegression()\n",
    "        lin_reg.fit(conf_pcs, ffa_data.T)  # regress noise PCs against each FFA voxel's timecourse\n",
    "        ffa_pred_noise = lin_reg.predict(conf_pcs)  # predicted noise in FFA (shape T x n_ffa_voxels)\n",
    "        ffa_comp = (ffa_data.T - ffa_pred_noise).T  # FFA with compCor noise removed\n",
    "    # At this point:\n",
    "    # func_gm (ROI signals), func_cf (noise signals), ffa_data (original FFA ROI signals), ffa_comp (FFA after baseline noise removal)\n",
    "    # conf (motion confounds if available), face_reg, place_reg are all prepared.\n",
    "    return func_gm, func_cf, conf, ffa_data, face_reg, place_reg, ffa_comp\n",
    "\n",
    "def train_cvae_model(obs_list: np.ndarray, noi_list: np.ndarray, conf: np.ndarray, ffa: np.ndarray, cvae_params: dict):\n",
    "    \"\"\"\n",
    "    Train the CVAE model on provided ROI (obs_list) and RONI (noi_list) data.\n",
    "    Parameters:\n",
    "      obs_list (np.ndarray): ROI data, shape (n_vox_roi, n_time)\n",
    "      noi_list (np.ndarray): RONI data, shape (n_vox_noise, n_time)\n",
    "      conf (np.ndarray): Confound matrix, shape (n_confounds, n_time)\n",
    "      ffa (np.ndarray): FFA ROI data, shape (n_ffa_voxels, n_time)\n",
    "      cvae_params (dict): Dictionary of CVAE hyperparameters (latent_dim, epochs, batch_size, learning_rate, beta, etc.)\n",
    "    Returns:\n",
    "      model (cVAE): Trained CVAE model.\n",
    "      track (dict): Training history with loss components and metrics per epoch.\n",
    "      outputs (dict): Dictionary of outputs (reconstructed signals, separated signals, etc.) after training.\n",
    "    \"\"\"\n",
    "    # Unpack hyperparameters\n",
    "    latent_dim = cvae_params.get(\"latent_dim\", (16,16))\n",
    "    epochs = cvae_params.get(\"epochs\", 50)\n",
    "    batch_size = cvae_params.get(\"batch_size\", 256)\n",
    "    lr = cvae_params.get(\"learning_rate\", 1e-3)\n",
    "    beta = cvae_params.get(\"beta\", 1.0)\n",
    "    gamma = cvae_params.get(\"gamma\", 1.0)\n",
    "    delta = cvae_params.get(\"delta\", 1.0)\n",
    "    # Prepare PyTorch device (CPU or GPU)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # Prepare training DataLoader\n",
    "    train_dataset = TrainDataset(obs_list, noi_list)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, drop_last=True)\n",
    "    # Prepare confounds tensor for conditioning in model (repeat for batch)\n",
    "    n_confounds, T = conf.shape\n",
    "    # Create a confound tensor of shape (batch_size, n_confounds, T) by repeating conf across the batch dimension\n",
    "    conf_batch = torch.tensor(np.array([conf for _ in range(batch_size)]), dtype=torch.float32).to(device)\n",
    "    # Initialize model and optimizer\n",
    "    model = cVAE(conf_batch, in_channels=4, in_dim=T, latent_dim=latent_dim,\n",
    "                 beta=beta, gamma=gamma, delta=delta).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # Tracking dictionary for losses and metrics\n",
    "    track = {\n",
    "        'l': [], 'kld_loss': [], 'recons_loss_roi': [], 'recons_loss_roni': [], \n",
    "        'loss_recon_conf_s': [], 'loss_recon_conf_z': [], 'ncc_loss_tg': [], 'ncc_loss_bg': [], \n",
    "        'ncc_loss_conf_s': [], 'ncc_loss_conf_z': [], 'smoothness_loss': [], 'recons_loss_fg': [],\n",
    "        'r_ffa_reg': [], 'r_compcor_reg': [], 'r_TG_reg': [], 'r_FG_reg': [], 'r_BG_reg': [],\n",
    "        'varexp': [], 'batch_varexp': [], 'ffa_io': [],\n",
    "        'tg_mu_z': [], 'tg_log_var_z': [], 'tg_mu_s': [], 'tg_log_var_s': []\n",
    "    }\n",
    "    # Pre-calculate baseline correlation metrics for FFA region\n",
    "    # r_ffa_reg: correlation between original FFA average and face regressor\n",
    "    # r_compcor_reg: correlation between baseline-compcorr-cleaned FFA average and face regressor\n",
    "    # (Note: These will remain constant each epoch, but we append them for reference.)\n",
    "    if 'face_reg' in globals():\n",
    "        # Compute these only if face_reg is provided in global scope (or pass as parameter if needed)\n",
    "        face_reg = globals().get('face_reg', None)\n",
    "    else:\n",
    "        face_reg = None\n",
    "    if face_reg is not None and face_reg.shape[0] == ffa.shape[1]:\n",
    "        ffa_mean = ffa.mean(axis=0)\n",
    "        ffa_compcorr_mean = None\n",
    "        # If baseline compcorr data is available in global outputs (for simplicity, pass ffa_compcorr via global if needed)\n",
    "        ffa_compcorr_data = globals().get('ffa_compcorr', None)\n",
    "        if ffa_compcorr_data is not None and ffa_compcorr_data.shape == ffa.shape:\n",
    "            ffa_compcorr_mean = ffa_compcorr_data.mean(axis=0)\n",
    "        else:\n",
    "            ffa_compcorr_mean = ffa_mean  # if not available, treat as same\n",
    "        base_r = np.corrcoef(ffa_mean, face_reg)[0, 1] if np.std(ffa_mean) > 0 else 0.0\n",
    "        compcorr_r = np.corrcoef(ffa_compcorr_mean, face_reg)[0, 1] if np.std(ffa_compcorr_mean) > 0 else 0.0\n",
    "    else:\n",
    "        base_r = compcorr_r = 0.0\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for obs_batch, noise_batch in train_loader:\n",
    "            # obs_batch: (batch, T), noise_batch: (batch, T) as float32\n",
    "            # Convert to torch and add coordinate channels\n",
    "            # We need to reshape each batch to (batch, channels=4, time) where channels = [signal, x_coord, y_coord, z_coord]\n",
    "            # To do that, we'll retrieve coordinates corresponding to indices in the dataset.\n",
    "            # Our TrainDataset currently pairs obs and noise by the same index, but their coordinates differ.\n",
    "            # For simplicity, we will use ROI coordinates for obs and a placeholder for noise (not used in model input).\n",
    "            batch_indices = np.arange(obs_batch.shape[0])\n",
    "            obs_coords = torch.tensor(gm_coords[batch_indices], dtype=torch.float32)  # shape (batch, 3)\n",
    "            # Repeat coordinates across time dimension\n",
    "            coord_times = obs_coords.unsqueeze(2).repeat(1, 1, obs_batch.shape[1])  # (batch, 3, T)\n",
    "            obs_ts = obs_batch.unsqueeze(1)  # (batch, 1, T)\n",
    "            obs_with_coords = torch.cat([obs_ts, coord_times], dim=1)  # (batch, 4, T)\n",
    "            # Do the same for noise batch using cf_coords\n",
    "            noise_coords = torch.tensor(cf_coords[batch_indices], dtype=torch.float32)\n",
    "            coord_times_noise = noise_coords.unsqueeze(2).repeat(1, 1, noise_batch.shape[1])\n",
    "            noise_ts = noise_batch.unsqueeze(1)\n",
    "            noise_with_coords = torch.cat([noise_ts, coord_times_noise], dim=1)\n",
    "            obs_with_coords = obs_with_coords.to(device)\n",
    "            noise_with_coords = noise_with_coords.to(device)\n",
    "            # Ensure confounds tensor in model has correct batch size (if last batch dropped, conf_batch already correct size)\n",
    "            # Forward pass: get both target and background outputs\n",
    "            outputs_tg = model.forward_tg(obs_with_coords)\n",
    "            outputs_bg = model.forward_bg(noise_with_coords)\n",
    "            loss_dict = model.loss_function(*outputs_tg, *outputs_bg)\n",
    "            loss = loss_dict['loss']\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        # Evaluate on full FFA region (signal-of-interest ROI) after epoch\n",
    "        model.eval()\n",
    "        # Prepare full FFA data as input (with coordinates)\n",
    "        ffa_coords = gm_coords[np.isin(gm_coords, gm_coords[ffa_survived], axis=0)]\n",
    "        # Actually, to get FFA coords corresponding to ffa (after filtering) we have ffa_survived boolean used above\n",
    "        ffa_coords = gm_coords[ffa_survived]\n",
    "        ffa_inputs = torch.tensor(ffa, dtype=torch.float32).unsqueeze(1)  # (n_ffa_vox, 1, T)\n",
    "        ffa_coord_stack = torch.tensor(ffa_coords, dtype=torch.float32)\n",
    "        ffa_coord_stack = ffa_coord_stack.unsqueeze(1).repeat(1, ffa_inputs.shape[-1], 1)  # (n_ffa_vox, T, 3)\n",
    "        ffa_coord_stack = ffa_coord_stack.permute(0, 2, 1)  # (n_ffa_vox, 3, T)\n",
    "        ffa_inputs_with_coords = torch.cat([ffa_inputs, ffa_coord_stack], dim=1).to(device)  # (n_ffa_vox, 4, T)\n",
    "        # Forward through model for FFA ROI\n",
    "        with torch.no_grad():\n",
    "            recon = model.forward_tg(ffa_inputs_with_coords)[0].cpu().numpy()  # reconstructed full signal (ROI)\n",
    "            fg = model.forward_fg(ffa_inputs_with_coords)[0].cpu().numpy()     # foreground (signal component)\n",
    "            bg = model.forward_bg(ffa_inputs_with_coords)[0].cpu().numpy()     # background (noise component)\n",
    "            # Compute metrics on FFA outputs\n",
    "            ffa_mean = ffa.mean(axis=0)\n",
    "            recon_mean = recon.mean(axis=0)\n",
    "            fg_mean = fg.mean(axis=0)\n",
    "            bg_mean = bg.mean(axis=0)\n",
    "            # Correlations with face_reg if available\n",
    "            if face_reg is not None:\n",
    "                r_ffa = np.corrcoef(ffa_mean, face_reg)[0, 1] if np.std(ffa_mean) > 0 else 0.0\n",
    "                r_tg = np.corrcoef(recon_mean, face_reg)[0, 1] if np.std(recon_mean) > 0 else 0.0\n",
    "                r_fg = np.corrcoef(fg_mean, face_reg)[0, 1] if np.std(fg_mean) > 0 else 0.0\n",
    "                r_bg = np.corrcoef(bg_mean, face_reg)[0, 1] if np.std(bg_mean) > 0 else 0.0\n",
    "            else:\n",
    "                r_ffa = r_tg = r_fg = r_bg = 0.0\n",
    "            # Variance explained (R^2) for FFA signals by reconstruction\n",
    "            SST = np.sum((ffa - ffa_mean) ** 2)\n",
    "            SSE = np.sum((ffa - recon) ** 2)\n",
    "            varexp = 1 - (SSE / (SST + 1e-8))\n",
    "            # Correlation between FFA input and reconstruction (averaged if multiple voxels)\n",
    "            c_io = np.corrcoef(ffa_mean, recon_mean)[0, 1] if np.std(ffa_mean) > 0 else 0.0\n",
    "        # Record epoch metrics in track\n",
    "        track['l'].append(epoch_loss / len(train_loader))\n",
    "        track['kld_loss'].append(float(loss_dict['kld_loss'].cpu().detach().numpy()))\n",
    "        track['recons_loss_roi'].append(float(loss_dict['recons_loss_roi'].cpu().detach().numpy()))\n",
    "        track['recons_loss_roni'].append(float(loss_dict['recons_loss_roni'].cpu().detach().numpy()))\n",
    "        track['loss_recon_conf_s'].append(float(loss_dict['loss_recon_conf_s'].cpu().detach().numpy()))\n",
    "        track['loss_recon_conf_z'].append(float(loss_dict['loss_recon_conf_z'].cpu().detach().numpy()))\n",
    "        track['ncc_loss_tg'].append(float(loss_dict['ncc_loss_tg'].cpu().detach().numpy()))\n",
    "        track['ncc_loss_bg'].append(float(loss_dict['ncc_loss_bg'].cpu().detach().numpy()))\n",
    "        track['ncc_loss_conf_s'].append(float(loss_dict['ncc_loss_conf_s'] if isinstance(loss_dict['ncc_loss_conf_s'], float) else loss_dict['ncc_loss_conf_s'].cpu().detach().numpy()))\n",
    "        track['ncc_loss_conf_z'].append(float(loss_dict['ncc_loss_conf_z'] if isinstance(loss_dict['ncc_loss_conf_z'], float) else loss_dict['ncc_loss_conf_z'].cpu().detach().numpy()))\n",
    "        track['smoothness_loss'].append(float(loss_dict['smoothness_loss'].cpu().detach().numpy()))\n",
    "        track['recons_loss_fg'].append(float(loss_dict['recons_loss_fg'].cpu().detach().numpy()))\n",
    "        # Append correlation metrics (constant baseline values each epoch for baseline lines)\n",
    "        track['r_ffa_reg'].append(base_r if 'base_r' in locals() else r_ffa)\n",
    "        track['r_compcor_reg'].append(compcorr_r if 'compcorr_r' in locals() else r_ffa)  # if no baseline, use r_ffa\n",
    "        track['r_TG_reg'].append(r_tg)\n",
    "        track['r_FG_reg'].append(r_fg)\n",
    "        track['r_BG_reg'].append(r_bg)\n",
    "        track['varexp'].append(varexp)\n",
    "        track['batch_varexp'].append(varexp)\n",
    "        track['ffa_io'].append(c_io)\n",
    "        # Track latent distributions (use mean of absolute values as summary)\n",
    "        track['tg_mu_z'].append(float(np.mean(np.abs(tg_mu_z.cpu().detach().numpy()))))\n",
    "        track['tg_log_var_z'].append(float(np.mean(np.abs(tg_log_var_z.cpu().detach().numpy()))))\n",
    "        track['tg_mu_s'].append(float(np.mean(np.abs(tg_mu_s.cpu().detach().numpy()))))\n",
    "        track['tg_log_var_s'].append(float(np.mean(np.abs(tg_log_var_s.cpu().detach().numpy()))))\n",
    "        # (Note: We reuse tg_mu_z etc from last batch forward in loop above; ideally compute on full data, but using last batch as approximation)\n",
    "        # Print progress message for epoch\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {track['l'][-1]:.4f} - FFA vs reg: {track['r_ffa_reg'][-1]:.3f}, CompCor vs reg: {track['r_compcor_reg'][-1]:.3f}, FG vs reg: {r_fg:.3f}, BG vs reg: {r_bg:.3f}\")\n",
    "    # After training, prepare outputs\n",
    "    outputs = {\n",
    "        'recon': recon,             # reconstructed full FFA signals (np.ndarray shape [n_ffa_voxels, T])\n",
    "        'signal': fg,               # foreground signals (FFA, shape [n_ffa_voxels, T])\n",
    "        'noise': bg,                # background signals (FFA, shape [n_ffa_voxels, T])\n",
    "        'ffa': ffa,                 # original FFA signals (input to model)\n",
    "        'ffa_compcorr': globals().get('ffa_compcorr', ffa),  # baseline compcorr cleaned FFA (if computed)\n",
    "        'face_reg': globals().get('face_reg', None),\n",
    "        'place_reg': globals().get('place_reg', None),\n",
    "        'confounds': conf\n",
    "    }\n",
    "    return model, track, outputs\n",
    "\n",
    "def plot_dashboard(track: dict, outputs: dict, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Plot a dashboard of training metrics and model outputs similar to the original script.\n",
    "    Saves the figure if output_dir is provided.\n",
    "    \"\"\"\n",
    "    # Setting up subplots grid (5 rows x 9 columns as in original)\n",
    "    nrows, ncols = 5, 9\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows))\n",
    "    axes = axes.flatten()\n",
    "    sp = 0\n",
    "    # Plot training losses over epochs\n",
    "    axes[sp].plot(track['l']); axes[sp].set_title(f\"Total loss: {track['l'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['batch_varexp']); axes[sp].set_title(f\"batch_varexp: {track['batch_varexp'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['varexp']); axes[sp].set_title(f\"FFA varexp: {track['varexp'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['ffa_io']); axes[sp].set_title(f\"ffa_io (corr FFA vs recon): {track['ffa_io'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['recons_loss_roi']); axes[sp].set_title(f\"recons_loss_roi: {track['recons_loss_roi'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['recons_loss_roni']); axes[sp].set_title(f\"recons_loss_roni: {track['recons_loss_roni'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['loss_recon_conf_s']); axes[sp].set_title(f\"loss_recon_conf_s: {track['loss_recon_conf_s'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['kld_loss']); axes[sp].set_title(f\"kld_loss: {track['kld_loss'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['loss_recon_conf_z']); axes[sp].set_title(f\"loss_recon_conf_z: {track['loss_recon_conf_z'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['ncc_loss_tg']); axes[sp].set_title(f\"ncc_loss_tg: {track['ncc_loss_tg'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['ncc_loss_bg']); axes[sp].set_title(f\"ncc_loss_bg: {track['ncc_loss_bg'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['recons_loss_fg']); axes[sp].set_title(f\"recons_loss_fg: {track['recons_loss_fg'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['ncc_loss_conf_s']); axes[sp].set_title(f\"ncc_loss_conf_s: {track['ncc_loss_conf_s'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['ncc_loss_conf_z']); axes[sp].set_title(f\"ncc_loss_conf_z: {track['ncc_loss_conf_z'][-1]:.2f}\"); sp += 1\n",
    "    axes[sp].plot(track['smoothness_loss']); axes[sp].set_title(f\"smoothness_loss: {track['smoothness_loss'][-1]:.2f}\"); sp += 1\n",
    "    # Plot an example voxel timecourse from a batch (last epoch, first voxel in batch vs recon FG/BG)\n",
    "    # (For simplicity, we'll use the last epoch first voxel of last batch from track if available)\n",
    "    if 'outputs' in globals():\n",
    "        # If we had stored batch_in and batch_out for last iteration in training, we could use them.\n",
    "        # For now, skip this specific plot due to lack of batch sample retention.\n",
    "        batch_obs_example = outputs['ffa'][0] if outputs['ffa'].shape[0] > 0 else None\n",
    "        batch_recon_example = outputs['recon'][0] if outputs['recon'].shape[0] > 0 else None\n",
    "        if batch_obs_example is not None and batch_recon_example is not None:\n",
    "            axes[sp].plot(batch_obs_example, 'b-')\n",
    "            axes[sp].plot(batch_recon_example, 'g-')\n",
    "            axes[sp].set_title(\"Batch example ROI vs recon\")\n",
    "        sp += 1\n",
    "    else:\n",
    "        sp += 1\n",
    "    # Plot FFA region average timecourse vs model recon average\n",
    "    axes[sp].plot(outputs['ffa'].mean(axis=0), label='FFA (orig)')\n",
    "    axes[sp].plot(outputs['recon'].mean(axis=0), label='Recon')\n",
    "    axes[sp].set_title(\"FFA AVG vs Recon AVG\")\n",
    "    axes[sp].legend(); sp += 1\n",
    "    # Plot FFA average vs model foreground (signal) and face task regressor\n",
    "    face_reg = outputs.get('face_reg', None)\n",
    "    axes[sp].plot(outputs['ffa'].mean(axis=0), 'k-', label='FFA')\n",
    "    axes[sp].plot(outputs['signal'].mean(axis=0), 'g-', label='Signal (FG)')\n",
    "    if face_reg is not None:\n",
    "        axes[sp].plot(face_reg, 'r--', label='Face_reg')\n",
    "    axes[sp].set_title(\"FFA SIGNAL vs Face reg\"); axes[sp].legend(); sp += 1\n",
    "    # Plot FFA average vs model background (noise) and face regressor\n",
    "    axes[sp].plot(outputs['ffa'].mean(axis=0), 'k-', label='FFA')\n",
    "    axes[sp].plot(outputs['noise'].mean(axis=0), 'r-', label='Noise (BG)')\n",
    "    if face_reg is not None:\n",
    "        axes[sp].plot(face_reg, 'b--', label='Face_reg')\n",
    "    axes[sp].set_title(\"FFA NOISE vs Face reg\"); axes[sp].legend(); sp += 1\n",
    "    # Plot example confound prediction (from z and s) vs actual confound (take one confound index, e.g., 2 for rot_z or similar if exists)\n",
    "    # If confounds exist\n",
    "    conf = outputs.get('confounds', None)\n",
    "    if conf is not None and conf.shape[0] >= 1:\n",
    "        conf_idx = min(conf.shape[0]-1, 2)  # pick one confound (e.g., 3rd if exists)\n",
    "        # We can attempt to get conf_pred from model's last processed latents (if saved in track or accessible)\n",
    "        # We did not store conf_pred explicitly; skipping detailed conf plot for simplicity.\n",
    "        axes[sp].plot(conf[conf_idx, :], 'k-', label='Confound actual')\n",
    "        axes[sp].set_title(\"Confound example\"); sp += 1\n",
    "    else:\n",
    "        sp += 1\n",
    "    # Plot track of correlation metrics over epochs\n",
    "    axes[sp].plot(track['r_ffa_reg'], 'k-', label='FFA raw')\n",
    "    axes[sp].plot(track['r_TG_reg'], 'b-', label='Recon (TG)')\n",
    "    axes[sp].set_title(f\"R TG-REG final: {track['r_TG_reg'][-1]:.2f}\")\n",
    "    axes[sp].legend(); sp += 1\n",
    "    axes[sp].plot(track['r_ffa_reg'], 'k-', label='FFA raw')\n",
    "    axes[sp].plot(track['r_compcor_reg'], 'b-', label='FFA CompCor')\n",
    "    axes[sp].plot(track['r_FG_reg'], 'g-', label='FG')\n",
    "    axes[sp].set_title(f\"R FG-REG final: {track['r_FG_reg'][-1]:.2f}\")\n",
    "    axes[sp].legend(); sp += 1\n",
    "    axes[sp].plot(track['r_ffa_reg'], 'k-', label='FFA raw')\n",
    "    axes[sp].plot(track['r_BG_reg'], 'r-', label='BG')\n",
    "    axes[sp].set_title(f\"R BG-REG final: {track['r_BG_reg'][-1]:.2f}\")\n",
    "    axes[sp].legend(); sp += 1\n",
    "    # Remove any unused subplots (in case we didn't fill all 45 slots)\n",
    "    for j in range(sp, nrows*ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "    fig.tight_layout()\n",
    "    if output_dir:\n",
    "        fig_path = os.path.join(output_dir, \"dashboard.png\")\n",
    "        plt.savefig(fig_path)\n",
    "        print(f\"Dashboard plot saved to {fig_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_outputs(model: cVAE, track: dict, outputs: dict, output_dir: str):\n",
    "    \"\"\"\n",
    "    Save model outputs and derivatives to files in the specified output directory.\n",
    "    - Saves track and outputs dictionaries as pickled files.\n",
    "    - Saves model state dictionary.\n",
    "    - Saves a 4D NIfTI image of the denoised signal (foreground) across the brain ROI.\n",
    "    \"\"\"\n",
    "    safe_mkdir(output_dir)\n",
    "    # Save training history and outputs as pickles\n",
    "    track_path = os.path.join(output_dir, \"training_track.pkl\")\n",
    "    outputs_path = os.path.join(output_dir, \"outputs.pkl\")\n",
    "    with open(track_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(track, f)\n",
    "    with open(outputs_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(outputs, f)\n",
    "    print(f\"Saved training track to {track_path}\")\n",
    "    print(f\"Saved outputs to {outputs_path}\")\n",
    "    # Save model weights\n",
    "    model_path = os.path.join(output_dir, \"cvae_model_state.pth\")\n",
    "    torch.save({'model_state_dict': model.state_dict()}, model_path)\n",
    "    print(f\"Saved model state to {model_path}\")\n",
    "    # Save the denoised signal as a NIfTI image (4D)\n",
    "    try:\n",
    "        import ants\n",
    "        # We will reconstruct a 4D image for \"signal\" output across the whole brain ROI.\n",
    "        # We have outputs['signal'] for FFA region only. For full ROI, we use model to generate signals for all ROI voxels.\n",
    "        # Let's generate the full ROI \"foreground signal\" image.\n",
    "        obs_arr = outputs.get('signal')  # currently only FFA region\n",
    "        # Instead, generate for all ROI:\n",
    "        if obs_arr is None or obs_arr.shape[0] != outputs['ffa'].shape[0]:\n",
    "            # If outputs['signal'] is only FFA, regenerate using model for all ROI voxels (obs_list)\n",
    "            obs_list = outputs['ffa'] if outputs.get('ffa') is not None else None\n",
    "            if obs_list is None:\n",
    "                obs_list = outputs['recon']  # fallback to recon if ffa not present\n",
    "            if obs_list is None:\n",
    "                print(\"No ROI data available to reconstruct full signal image.\")\n",
    "            else:\n",
    "                # Need to incorporate coordinates for each ROI voxel\n",
    "                full_obs = torch.tensor(obs_list, dtype=torch.float32).unsqueeze(1)  # (N_roi, 1, T)\n",
    "                gm_coord_tensor = torch.tensor(gm_coords[:obs_list.shape[0]], dtype=torch.float32)  # coordinates for ROI voxels (assuming original gm_coords aligns with obs_list)\n",
    "                coord_stack = gm_coord_tensor.unsqueeze(1).repeat(1, obs_list.shape[1], 1).permute(0, 2, 1)  # (N_roi, 3, T)\n",
    "                full_input = torch.cat([full_obs, coord_stack], dim=1).to(next(model.parameters()).device)\n",
    "                with torch.no_grad():\n",
    "                    fg_full = model.forward_fg(full_input)[0].cpu().numpy()  # (N_roi, 4, T) but only first channel is signal\n",
    "                fg_signal = fg_full[:, 0, :]  # shape (N_roi, T)\n",
    "                # Create an array for full brain with shape (X*Y*Z, T)\n",
    "                brain_signal_flat = np.zeros((gm_mask.size, fg_signal.shape[1]), dtype=np.float32)\n",
    "                brain_signal_flat[gm_flat, :] = 0  # ensure shape\n",
    "                # Place reconstructed signals into their ROI locations\n",
    "                brain_signal_flat[gm_flat, :] = fg_signal\n",
    "                # Reshape to original image 4D shape\n",
    "                brain_signal_img_data = brain_signal_flat.T.reshape(epi_img.numpy().shape)\n",
    "        else:\n",
    "            # If outputs['signal'] covered ROI fully (which it likely doesn't, as it's just FFA subset), handle directly\n",
    "            brain_signal_img_data = np.zeros_like(epi_img.numpy())\n",
    "            brain_signal_img_data = brain_signal_img_data.astype(np.float32)\n",
    "            brain_signal_img_data[gm_mask] = outputs['signal'].flatten()\n",
    "        # Save using ANTs image\n",
    "        signal_img = epi_img.new_image_like(brain_signal_img_data)\n",
    "        signal_img_path = os.path.join(output_dir, \"signal_denoised.nii.gz\")\n",
    "        signal_img.to_file(signal_img_path)\n",
    "        print(f\"Saved denoised signal image to {signal_img_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save NIfTI signal image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ece92b-2420-4ffa-810a-1cb3f7dc44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Main execution (if run as script)\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    obs_list, noi_list, conf, ffa, face_reg, place_reg, ffa_compcorr = load_data_and_preprocess(epi_path, anat_path, n_dummy)\n",
    "    # For convenience, set global variables for face_reg, place_reg, ffa_compcorr to use in training metrics\n",
    "    globals()['face_reg'] = face_reg\n",
    "    globals()['place_reg'] = place_reg\n",
    "    globals()['ffa_compcorr'] = ffa_compcorr\n",
    "    # Train CVAE model\n",
    "    model, track, outputs = train_cvae_model(obs_list, noi_list, conf, ffa, cvae_params)\n",
    "    # Plot results dashboard and save it\n",
    "    plot_dashboard(track, outputs, output_dir=ofn_root)\n",
    "    # Save outputs and model\n",
    "    save_outputs(model, track, outputs, output_dir=ofn_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5d543f-a69b-46f6-99c4-255141661d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epi_path = '../Data/020-fmriprepped/sub-NDARINV1H7JEJW1/ses-baselineYear1Arm1/func/sub-NDARINV1H7JEJW1_ses-baselineYear1Arm1_task-nback_run-02_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz'\n",
    "anat_path = \n",
    "n_dummy = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aeb092-48ef-4b82-a672-7f1c664d1834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cad09c-0a64-4996-89df-f9bc3317efe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e5d38-00eb-4918-8580-e095b8e15864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
